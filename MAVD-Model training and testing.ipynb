{"cells":[{"cell_type":"markdown","source":"# Urban Noise Monitoring","metadata":{"cell_id":"870b76cb6e884298852a862f1225b619","formattedRanges":[],"deepnote_cell_type":"text-cell-h1"}},{"cell_type":"markdown","source":"In this notebook we implement a Deep Learning model for the Urban noise monitoring task. In particular our model has to be able to recognize three different vehicles (car, bus, and motorcycle) only basing on audio data. In the first section we setup the notebook importing all the dependencies. Then we load and preprocess the data. In the third section we build and train the model. In the last section we will test the selected model and finally we save it.","metadata":{"cell_id":"559208a16065485aa22fa15d7316355a","formattedRanges":[],"deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":"## Notebook Setup","metadata":{"cell_id":"dfb52f76396344c2bd71d6624d19942c","formattedRanges":[],"deepnote_cell_type":"text-cell-h2"}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.metrics import *\nimport tensorflow_addons as tfa\nimport tensorflow_model_optimization as tfmot\n\nfrom glob import glob\n\nimport numpy as np\n\n\nfrom MAVD_Preprocessing import *\nfrom functools import partial\ntf.compat.v1.enable_eager_execution()","metadata":{"tags":[],"cell_id":"5977a88e11c04a31b2629525c18338c6","source_hash":"d104d032","execution_start":1681227944825,"execution_millis":28,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":11},{"cell_type":"code","source":"import os\nimport numpy as np\nimport random\n\nseed = 42\nos.environ['PYTHONHASHSEED'] = str(seed)\nos.environ['TF_DETERMINISTIC_OPS'] = '1'\nrandom.seed(seed)\ntf.random.set_seed(seed)\nnp.random.seed(seed)\n","metadata":{"tags":[],"cell_id":"5f3cc060a23b4e8d83503c04b5dd9887","source_hash":"32724c28","execution_start":1681227944878,"execution_millis":6,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Load the TensorBoard notebook extension\n%load_ext tensorboard\n\nimport datetime\n\nMODEL_NAME = \"hmw_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n\nlog_dir = f\"logs/fit/{MODEL_NAME}\"\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\nMODEL_NAME","metadata":{"cell_id":"1ca5220ce9a34540a19535442efcecfd","source_hash":"95893555","execution_start":1681227944892,"execution_millis":9,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"The tensorboard extension is already loaded. To reload it, use:\n  %reload_ext tensorboard\n","output_type":"stream"},{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"'hmw_20230411-154544'"},"metadata":{}}],"execution_count":13},{"cell_type":"markdown","source":"## Data import and preprocessing","metadata":{"cell_id":"9c197a291cbf4f02a320b55801f65e55","formattedRanges":[],"deepnote_cell_type":"text-cell-h2"}},{"cell_type":"code","source":"#Select the target labels\nLABELS = ['car','bus','motorcycle']#,'other','truck','chatter']\nnum_classes = len(LABELS)\n#Select the task and the corrispsonding variables \ntask = 'multilabel' # Select 'multilabel' or 'singlelabel'\nlast_activation = 'softmax' if task == 'singlelabel' else 'sigmoid'\nthreshold = None if task == 'singlelabel' else 0.5\n\n#Setupt the training parameters\nTRAINING_ARGS = {\n    'batch_size': 32,\n    'initial_learning_rate': 0.01,\n    'end_learning_rate': 1.e-6,\n    'epochs': 10\n}\n\n#Setup the preprocessing parameters\nPREPROCESSING_MFCCS_ARGS = {\n    'downsampling_rate': 44100,\n    'frame_length_in_s': 0.032,\n    'frame_step_in_s': 0.008,\n    'num_mel_bins' : 128,\n    'lower_frequency': 0,\n    'upper_frequency': 2000,\n    'num_coefficients':10\n}\n\n### Setup the optimization parameters\n#Width multiplier\nalpha = 0.25       \n#Weight pruning\nweight_pruning = True\nfinal_sparsity = 0.7  \n#Depthwise\ndepthwise = False     \n\n\n\nbatch_size = TRAINING_ARGS['batch_size']\nepochs = TRAINING_ARGS['epochs']","metadata":{"tags":[],"cell_id":"94a7e6a2365e484fb86d1690900249d0","source_hash":"ea3c19fa","execution_start":1681227944918,"execution_millis":13,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"Prepare the preprocessing functions","metadata":{"cell_id":"5ccbbc73be364163ae4e9725b72872aa","formattedRanges":[],"deepnote_cell_type":"text-cell-p"}},{"cell_type":"code","source":"get_frozen_mfccs = partial(get_mfccs, \n**PREPROCESSING_MFCCS_ARGS)\n\ndef preprocess_mfccs_single(filename):\n    signal, label = get_frozen_mfccs(filename)\n    signal = tf.expand_dims(signal, -1)\n    print(label)\n    label = [label == elem for elem in LABELS]\n    print(label)\n    label_id = tf.cast(label, dtype=tf.int32)\n    label_id = tf.reshape(label_id, shape=(-1,))\n    print(label_id)\n    return signal, label_id\n\ndef preprocess_mfccs_multi(filename):\n    signal, label = get_frozen_mfccs(filename)\n    signal = tf.expand_dims(signal, -1)\n    label_id = tf.strings.to_number(label, out_type=tf.int32)\n    return signal, label_id","metadata":{"tags":[],"cell_id":"ef44475d02c64cae9095f3a07ba9d99c","source_hash":"929082d0","execution_start":1681227944933,"execution_millis":6,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":15},{"cell_type":"code","source":"#Load the data\ntrain_ds = tf.data.Dataset.list_files(f'MAVD-traffic/{task}_train/*.flac')\nval_ds = tf.data.Dataset.list_files(f'MAVD-traffic/{task}_validate/*.flac')\ntest_ds = tf.data.Dataset.list_files(f'MAVD-traffic/{task}_test/*.flac')\n\n#train_ds = tf.data.Dataset.list_files([f'MAVD-traffic/{task}_train/car*',f'MAVD-traffic/{task}_train/motorcycle*',f'MAVD-traffic/{task}_train/bus*'])\n#val_ds = tf.data.Dataset.list_files([f'MAVD-traffic/{task}_validate/car*.flac',f'MAVD-traffic/{task}_validate/motorcycle*.flac',f'MAVD-traffic/{task}_validate/bus*.flac'])\n#test_ds = tf.data.Dataset.list_files([f'MAVD-traffic/{task}_test/car*.flac',f'MAVD-traffic/{task}_test/motorcycle*.flac',f'MAVD-traffic/{task}_test/bus*.flac'])\n\n#Be sure thath the number of data is a multiple of the batch size\ntrain_len = len(train_ds)\nval_len = len(val_ds)\ntest_len = len(test_ds)\n\ntrain_len_adj = train_len - (train_len % batch_size)\nval_len_adj = val_len - (val_len % batch_size)\ntest_len_adj = test_len - (test_len % batch_size)\n\ntrain_ds_adj = train_ds.take(train_len_adj).shuffle(train_len_adj)\nval_ds_adj = val_ds.take(val_len_adj).shuffle(val_len_adj)\ntest_ds_adj = test_ds.take(test_len_adj).shuffle(test_len_adj)\n\nprint(f'Number of files in training set: {train_len}, Number of files in validation set: {val_len}, Number of files in test set: {test_len}')\nprint(f'Number of files in training set after adjusting for batch size: {train_len_adj}, Number of files in validation set after adjusting for batch size: {val_len_adj}, Number of files in test set after adjusting for batch size: {test_len_adj}')\n\n#Preprocess the data\nif task == 'singlelabel':\n    train_ds = train_ds_adj.map(preprocess_mfccs_single).batch(batch_size).cache()\n    val_ds = val_ds_adj.map(preprocess_mfccs_single).batch(batch_size)\n    val_ds = test_ds_adj.map(preprocess_mfccs_single).batch(batch_size)\nelif task == 'multilabel': \n    train_ds = train_ds_adj.map(preprocess_mfccs_multi).batch(batch_size).cache()\n    val_ds = val_ds_adj.map(preprocess_mfccs_multi).batch(batch_size)\n    test_ds = test_ds_adj.map(preprocess_mfccs_multi).batch(batch_size)\n\nprint(f'Number of batches in training set: {tf.data.experimental.cardinality(train_ds)}')\nprint(f'Number of batches in validation set: {tf.data.experimental.cardinality(val_ds)}')\nprint(f'Number of batches in test set: {tf.data.experimental.cardinality(test_ds)}')\n\n\nfor example_batch, example_labels in train_ds.take(1):\n    batch_size_check = example_batch.shape[0]\n    print(f'Batch Shape: {example_batch.shape}, Data Shape: {example_batch.shape[1:]}, Labels: {example_labels}')\n    if batch_size_check != batch_size:\n        print(f'Warning: batch size is {batch_size_check}, which is different from the desired batch size of {batch_size}.')\n","metadata":{"tags":[],"cell_id":"1ed784320ea94089a8b6a31d789558eb","source_hash":"e4cc0d07","execution_start":1681227944962,"execution_millis":5140,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Number of files in training set: 6709, Number of files in validation set: 1995, Number of files in test set: 4990\nNumber of files in training set after adjusting for batch size: 6688, Number of files in validation set after adjusting for batch size: 1984, Number of files in test set after adjusting for batch size: 4960\n2023-04-11 15:45:45.910686: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2023-04-11 15:45:45.912776: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2023-04-11 15:45:45.913087: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2023-04-11 15:45:46.336196: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2023-04-11 15:45:46.338274: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2023-04-11 15:45:46.338538: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2023-04-11 15:45:47.013123: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2023-04-11 15:45:47.015867: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\n2023-04-11 15:45:47.016220: W tensorflow/core/framework/op_kernel.cc:1780] OP_REQUIRES failed at functional_ops.cc:373 : INTERNAL: No function library\nNumber of batches in training set: 209\nNumber of batches in validation set: 62\nNumber of batches in test set: 155\nBatch Shape: (32, 122, 10, 1), Data Shape: (122, 10, 1), Labels: [[0 0 0]\n [0 0 0]\n [0 0 0]\n [1 0 0]\n [1 0 0]\n [0 0 0]\n [1 0 0]\n [1 0 0]\n [1 0 0]\n [1 0 0]\n [1 0 0]\n [1 0 0]\n [1 0 0]\n [0 1 0]\n [1 1 0]\n [1 1 0]\n [0 1 0]\n [1 0 1]\n [1 1 0]\n [1 0 0]\n [1 1 0]\n [0 0 0]\n [0 0 0]\n [1 1 0]\n [1 0 1]\n [1 0 0]\n [1 1 0]\n [1 0 0]\n [0 1 0]\n [1 1 0]\n [1 1 0]\n [0 1 1]]\n2023-04-11 15:45:49.961432: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"## Build the model","metadata":{"cell_id":"8d8ce1d1cee94945bc7add6088ec3518","formattedRanges":[],"deepnote_cell_type":"text-cell-h2"}},{"cell_type":"markdown","source":"Homeworks' model","metadata":{"cell_id":"fabc1af29a1e45b1b946d3c7bfc49f91","formattedRanges":[],"deepnote_cell_type":"text-cell-p"}},{"cell_type":"code","source":"model = tf.keras.Sequential([\n    tf.keras.layers.Input(shape=example_batch.shape[1:]),\n    \n    tf.keras.layers.Conv2D(filters=256*alpha, kernel_size=[3, 3], strides=[2, 2],\n        use_bias=False, padding='valid'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.ReLU(),\n\n    tf.keras.layers.DepthwiseConv2D(kernel_size=[3, 3], strides=[1, 1], \n        use_bias=False, padding='same'),\n    tf.keras.layers.Conv2D(filters=256*alpha, kernel_size=[1, 1], strides=[1, 1],   \n       use_bias=False),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.ReLU(),\n\n    tf.keras.layers.DepthwiseConv2D(kernel_size=[3, 3], strides=[1, 1],\n        use_bias=False, padding='same'),\n    tf.keras.layers.Conv2D(filters=256*alpha, kernel_size=[1, 1], strides=[1, 1],   \n       use_bias=False),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.ReLU(),\n\n    tf.keras.layers.GlobalAveragePooling2D(),\n    tf.keras.layers.Dense(units=len(LABELS), activation=last_activation)\n\n])\n\nmodel.summary()","metadata":{"cell_id":"bda22f574bd64a829b17eeae013da913","source_hash":"c962ece4","execution_start":1681227981608,"execution_millis":208,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Model: \"sequential_3\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d_12 (Conv2D)          (None, 60, 4, 64)         576       \n                                                                 \n batch_normalization_13 (Bat  (None, 60, 4, 64)        256       \n chNormalization)                                                \n                                                                 \n re_lu_6 (ReLU)              (None, 60, 4, 64)         0         \n                                                                 \n depthwise_conv2d_4 (Depthwi  (None, 60, 4, 64)        576       \n seConv2D)                                                       \n                                                                 \n conv2d_13 (Conv2D)          (None, 60, 4, 64)         4096      \n                                                                 \n batch_normalization_14 (Bat  (None, 60, 4, 64)        256       \n chNormalization)                                                \n                                                                 \n re_lu_7 (ReLU)              (None, 60, 4, 64)         0         \n                                                                 \n depthwise_conv2d_5 (Depthwi  (None, 60, 4, 64)        576       \n seConv2D)                                                       \n                                                                 \n conv2d_14 (Conv2D)          (None, 60, 4, 64)         4096      \n                                                                 \n batch_normalization_15 (Bat  (None, 60, 4, 64)        256       \n chNormalization)                                                \n                                                                 \n re_lu_8 (ReLU)              (None, 60, 4, 64)         0         \n                                                                 \n global_average_pooling2d_2   (None, 64)               0         \n (GlobalAveragePooling2D)                                        \n                                                                 \n dense_4 (Dense)             (None, 3)                 195       \n                                                                 \n=================================================================\nTotal params: 10,883\nTrainable params: 10,499\nNon-trainable params: 384\n_________________________________________________________________\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"if depthwise:\n    model = tf.keras.Sequential([\n        tf.keras.layers.Input(shape=example_batch.shape[1:]),\n\n        #Conv1\n        tf.keras.layers.Conv2D(64*alpha, (3, 3), padding='same', activation='relu'),\n        tf.keras.layers.BatchNormalization(),\n        \n        #Conv2 + pooling + dropout\n        tf.keras.layers.DepthwiseConv2D(kernel_size=[3, 3], strides=[1, 1], \n            use_bias=False, padding='same'),\n        tf.keras.layers.Conv2D(filters=64*alpha, kernel_size=[1, 1], strides=[1, 1],\n            use_bias=False, padding='same'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.MaxPooling2D((2,2)),\n        tf.keras.layers.Dropout(0.2),\n        \n        #Conv3\n        tf.keras.layers.DepthwiseConv2D(kernel_size=[3, 3], strides=[1, 1], \n            use_bias=False, padding='same'),\n        tf.keras.layers.Conv2D(128*alpha, kernel_size=[1, 1], strides=[1, 1], padding='same', activation='relu'),\n        tf.keras.layers.BatchNormalization(),\n\n        #Conv4 + pooling + dropout\n        tf.keras.layers.DepthwiseConv2D(kernel_size=[3, 3], strides=[1, 1], \n            use_bias=False, padding='same'),\n        tf.keras.layers.Conv2D(128*alpha, kernel_size=[1, 1], strides=[1, 1], padding='same', activation='relu'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.MaxPooling2D((2,2)),\n        tf.keras.layers.Dropout(0.3),\n        \n        #Conv5\n        tf.keras.layers.DepthwiseConv2D(kernel_size=[3, 3], strides=[1, 1], \n            use_bias=False, padding='same'),\n        tf.keras.layers.Conv2D(256*alpha, kernel_size=[1, 1], strides=[1, 1], padding='same', activation='relu'),\n        tf.keras.layers.BatchNormalization(),\n        \n        #Conv6 + pooling + dropout\n        tf.keras.layers.DepthwiseConv2D(kernel_size=[3, 3], strides=[1, 1], \n            use_bias=False, padding='same'),\n        tf.keras.layers.Conv2D(256*alpha, kernel_size=[1, 1], strides=[1, 1], padding='same', activation='relu'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.MaxPooling2D((2,2)),\n        tf.keras.layers.Dropout(0.4),\n\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(256, activation='relu'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.5),\n        tf.keras.layers.Dense(len(LABELS), activation=last_activation)\n    ])\nelse:\n    model = tf.keras.Sequential([\n        tf.keras.layers.Input(shape=example_batch.shape[1:]),\n\n        tf.keras.layers.Conv2D(64*alpha, (3, 3), padding='same', activation='relu'),\n        tf.keras.layers.BatchNormalization(),\n\n        tf.keras.layers.Conv2D(filters=64*alpha, kernel_size=[3, 3], strides=[1, 1],\n            use_bias=False, padding='same'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.MaxPooling2D((2,2)),\n        tf.keras.layers.Dropout(0.2),\n\n        tf.keras.layers.Conv2D(128*alpha, kernel_size=[3, 3], strides=[1, 1], padding='same', activation='relu'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Conv2D(128*alpha, kernel_size=[3, 3], strides=[1, 1], padding='same', activation='relu'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.MaxPooling2D((2,2)),\n        tf.keras.layers.Dropout(0.3),\n\n        tf.keras.layers.Conv2D(256*alpha, kernel_size=[3, 3], strides=[1, 1], padding='same', activation='relu'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Conv2D(256*alpha, kernel_size=[3, 3], strides=[1, 1], padding='same', activation='relu'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.MaxPooling2D((2,2)),\n        tf.keras.layers.Dropout(0.4),\n\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(256, activation='relu'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.Dropout(0.5),\n        tf.keras.layers.Dense(len(LABELS), activation=last_activation)\n    ])\nmodel.summary()\n","metadata":{"cell_id":"3b08df2dedb9402483a8afa4d3af9064","source_hash":"cc4a1a7c","execution_start":1681227385200,"execution_millis":523,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Model: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d_3 (Conv2D)           (None, 122, 10, 16)       160       \n                                                                 \n batch_normalization_3 (Batc  (None, 122, 10, 16)      64        \n hNormalization)                                                 \n                                                                 \n conv2d_4 (Conv2D)           (None, 122, 10, 16)       2304      \n                                                                 \n batch_normalization_4 (Batc  (None, 122, 10, 16)      64        \n hNormalization)                                                 \n                                                                 \n max_pooling2d (MaxPooling2D  (None, 61, 5, 16)        0         \n )                                                               \n                                                                 \n dropout (Dropout)           (None, 61, 5, 16)         0         \n                                                                 \n conv2d_5 (Conv2D)           (None, 61, 5, 32)         4640      \n                                                                 \n batch_normalization_5 (Batc  (None, 61, 5, 32)        128       \n hNormalization)                                                 \n                                                                 \n conv2d_6 (Conv2D)           (None, 61, 5, 32)         9248      \n                                                                 \n batch_normalization_6 (Batc  (None, 61, 5, 32)        128       \n hNormalization)                                                 \n                                                                 \n max_pooling2d_1 (MaxPooling  (None, 30, 2, 32)        0         \n 2D)                                                             \n                                                                 \n dropout_1 (Dropout)         (None, 30, 2, 32)         0         \n                                                                 \n conv2d_7 (Conv2D)           (None, 30, 2, 64)         18496     \n                                                                 \n batch_normalization_7 (Batc  (None, 30, 2, 64)        256       \n hNormalization)                                                 \n                                                                 \n conv2d_8 (Conv2D)           (None, 30, 2, 64)         36928     \n                                                                 \n batch_normalization_8 (Batc  (None, 30, 2, 64)        256       \n hNormalization)                                                 \n                                                                 \n max_pooling2d_2 (MaxPooling  (None, 15, 1, 64)        0         \n 2D)                                                             \n                                                                 \n dropout_2 (Dropout)         (None, 15, 1, 64)         0         \n                                                                 \n flatten (Flatten)           (None, 960)               0         \n                                                                 \n dense_1 (Dense)             (None, 256)               246016    \n                                                                 \n batch_normalization_9 (Batc  (None, 256)              1024      \n hNormalization)                                                 \n                                                                 \n dropout_3 (Dropout)         (None, 256)               0         \n                                                                 \n dense_2 (Dense)             (None, 3)                 771       \n                                                                 \n=================================================================\nTotal params: 320,483\nTrainable params: 319,523\nNon-trainable params: 960\n_________________________________________________________________\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"Prepare the model for weight pruning if required","metadata":{"cell_id":"ba1ef53100d244c59c3cc0ab052a3e0e","formattedRanges":[],"deepnote_cell_type":"text-cell-p"}},{"cell_type":"code","source":"if weight_pruning:\n    prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n\n    begin_step = int(len(train_ds) * epochs * 0.2)\n    end_step = int(len(train_ds) * epochs)\n\n    pruning_params = {\n        'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(\n            initial_sparsity=0.0,\n            final_sparsity=final_sparsity,\n            begin_step=begin_step,\n            end_step=end_step\n        )\n    }\n\n    model = prune_low_magnitude(model, **pruning_params)\n    print(\"Pruning added\")","metadata":{"tags":[],"cell_id":"21702a58245a42b9a6ccc1c01a8f745c","source_hash":"f0790755","execution_start":1681227988009,"execution_millis":221,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Pruning added\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"#Setup the loss function\nloss = tf.losses.BinaryCrossentropy(from_logits=False) if task == 'multilabel'\\\nelse tf.losses.CategoricalCrossentropy(from_logits=False)\n\n#Setup the linear decay for the learning rate\ninitial_learning_rate = TRAINING_ARGS['initial_learning_rate']\nend_learning_rate = TRAINING_ARGS['end_learning_rate']\n\nlinear_decay = tf.keras.optimizers.schedules.PolynomialDecay(\n    initial_learning_rate=initial_learning_rate,\n    end_learning_rate=end_learning_rate,\n    decay_steps=len(train_ds) * epochs,\n)\n\n#Setup the Optimizer\noptimizer = tf.optimizers.SGD(learning_rate=linear_decay)\n\n#Setup the metrics\nmetrics = [tfa.metrics.F1Score(num_classes=num_classes, \n                                average='micro',\n                                threshold=threshold,\n                                name = 'f1_micro'),\n            tfa.metrics.F1Score(num_classes=num_classes,\n             average='weighted', \n             threshold=threshold, \n             name = 'f1_weighted'),\n            ]\n\nmodel.compile(\n    loss=loss,\n    optimizer=optimizer,\n    metrics=metrics\n)\n\ncallbacks = [tensorboard_callback]\n\nif weight_pruning:\n    callbacks.append(tfmot.sparsity.keras.UpdatePruningStep(), )\n\nprint(f\"Task: {task}, Loss: {loss}, last_activation: {last_activation}\")\n\nhistory = model.fit(train_ds, \n                    epochs=epochs,\n                    validation_data=val_ds,\n                    callbacks=callbacks)\n\n#ATTENTION, if a Graph execution error is raised, please re-run the cell\n","metadata":{"tags":[],"cell_id":"2829706111e746e8ba3875e784f84e83","source_hash":"fe77d19a","execution_start":1681227991026,"execution_millis":2376857,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Task: multilabel, Loss: <keras.losses.BinaryCrossentropy object at 0x7fed992c0550>, last_activation: sigmoid\nEpoch 1/10\n209/209 [==============================] - 642s 3s/step - loss: 0.5600 - f1_micro: 0.6168 - f1_weighted: 0.4620 - val_loss: 0.6498 - val_f1_micro: 0.6293 - val_f1_weighted: 0.5075\nEpoch 2/10\n209/209 [==============================] - 160s 770ms/step - loss: 0.5441 - f1_micro: 0.6168 - f1_weighted: 0.4620 - val_loss: 0.5986 - val_f1_micro: 0.6205 - val_f1_weighted: 0.4647\nEpoch 3/10\n209/209 [==============================] - 220s 1s/step - loss: 0.5433 - f1_micro: 0.6168 - f1_weighted: 0.4620 - val_loss: 0.5594 - val_f1_micro: 0.6202 - val_f1_weighted: 0.4643\nEpoch 4/10\n209/209 [==============================] - 170s 819ms/step - loss: 0.5437 - f1_micro: 0.6168 - f1_weighted: 0.4620 - val_loss: 0.5933 - val_f1_micro: 0.6216 - val_f1_weighted: 0.4662\nEpoch 5/10\n209/209 [==============================] - 222s 1s/step - loss: 0.5451 - f1_micro: 0.6168 - f1_weighted: 0.4620 - val_loss: 0.5606 - val_f1_micro: 0.6202 - val_f1_weighted: 0.4644\nEpoch 6/10\n209/209 [==============================] - 161s 772ms/step - loss: 0.5471 - f1_micro: 0.6168 - f1_weighted: 0.4620 - val_loss: 0.5678 - val_f1_micro: 0.6393 - val_f1_weighted: 0.5644\nEpoch 7/10\n209/209 [==============================] - 220s 1s/step - loss: 0.5475 - f1_micro: 0.6168 - f1_weighted: 0.4620 - val_loss: 0.5518 - val_f1_micro: 0.6189 - val_f1_weighted: 0.4630\nEpoch 8/10\n209/209 [==============================] - 160s 771ms/step - loss: 0.5457 - f1_micro: 0.6168 - f1_weighted: 0.4620 - val_loss: 0.5454 - val_f1_micro: 0.6194 - val_f1_weighted: 0.4635\nEpoch 9/10\n209/209 [==============================] - 220s 1s/step - loss: 0.5438 - f1_micro: 0.6168 - f1_weighted: 0.4620 - val_loss: 0.5454 - val_f1_micro: 0.6194 - val_f1_weighted: 0.4635\nEpoch 10/10\n209/209 [==============================] - 161s 775ms/step - loss: 0.5438 - f1_micro: 0.6168 - f1_weighted: 0.4620 - val_loss: 0.5472 - val_f1_micro: 0.6196 - val_f1_weighted: 0.4637\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"## Final Sparsity Check","metadata":{"cell_id":"b25af33014a34519a888d6ab37a4860a","formattedRanges":[],"deepnote_cell_type":"text-cell-h2"}},{"cell_type":"code","source":"import numpy as np\n\n\nif weight_pruning:\n    for layer in model.layers:\n        if isinstance(layer, tf.keras.layers.Wrapper):\n            weights = layer.trainable_weights\n        else:\n            weights = layer.weights\n        for weight in weights:        \n            weight_size = weight.numpy().size\n            zero_num = np.count_nonzero(weight == 0)\n            print(\n                f'{weight.name}: {zero_num/weight_size:.2%} sparsity ',\n                f'({zero_num}/{weight_size})',\n            )\nelse:\n    print(\"Weight pruning not used\")","metadata":{"cell_id":"3a10e5af5a60453d851a070471aad149","source_hash":"492b092a","execution_start":1681230367901,"execution_millis":19,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"conv2d_12/kernel:0: 69.97% sparsity  (403/576)\nbatch_normalization_13/gamma:0: 0.00% sparsity  (0/64)\nbatch_normalization_13/beta:0: 0.00% sparsity  (0/64)\ndepthwise_conv2d_4/depthwise_kernel:0: 0.00% sparsity  (0/576)\nconv2d_13/kernel:0: 70.00% sparsity  (2867/4096)\nbatch_normalization_14/gamma:0: 0.00% sparsity  (0/64)\nbatch_normalization_14/beta:0: 0.00% sparsity  (0/64)\ndepthwise_conv2d_5/depthwise_kernel:0: 0.00% sparsity  (0/576)\nconv2d_14/kernel:0: 70.00% sparsity  (2867/4096)\nbatch_normalization_15/gamma:0: 0.00% sparsity  (0/64)\nbatch_normalization_15/beta:0: 0.00% sparsity  (0/64)\ndense_4/kernel:0: 69.79% sparsity  (134/192)\ndense_4/bias:0: 0.00% sparsity  (0/3)\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"## Model Testing","metadata":{"cell_id":"2bb06cb9065944938212ef97fcfb27cd","formattedRanges":[],"deepnote_cell_type":"text-cell-h2"}},{"cell_type":"code","source":"results = model.evaluate(test_ds)\nresults","metadata":{"cell_id":"e74b9ed4f0f147f2a5d31db4e7fe5ec4","source_hash":"193cf6b3","execution_start":1681230367925,"execution_millis":358732,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"155/155 [==============================] - 359s 2s/step - loss: 0.5455 - f1_micro: 0.6088 - f1_weighted: 0.4541\n","output_type":"stream"},{"output_type":"execute_result","execution_count":23,"data":{"text/plain":"[0.5454511046409607, 0.6088300347328186, 0.454133003950119]"},"metadata":{}}],"execution_count":23},{"cell_type":"markdown","source":"## Save the model","metadata":{"cell_id":"7f720bcbae904a2c9522807260d21ef8","formattedRanges":[],"deepnote_cell_type":"text-cell-h2"}},{"cell_type":"code","source":"MODEL_NAME","metadata":{"cell_id":"7044eac0bb5e4e9882d397ed19cf8512","source_hash":"ab18e5bc","execution_start":1681230726665,"execution_millis":38,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"output_type":"execute_result","execution_count":24,"data":{"text/plain":"'hmw_20230411-154544'"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"from time import time\nimport os\n\n\n#Strip the model before save it\nif weight_pruning:\n    model_for_export = tfmot.sparsity.keras.strip_pruning(model)\nelse:\n    model_for_export = model\nsaved_model_dir = f'./saved_models/{MODEL_NAME}'\nif not os.path.exists(saved_model_dir):\n    os.makedirs(saved_model_dir)\nmodel_for_export.save(saved_model_dir)\n\n#Prepare the folder for tflite models\ntflite_models_dir = './tflite_models'\nif not os.path.exists(tflite_models_dir):\n    os.makedirs(tflite_models_dir)","metadata":{"cell_id":"4926bdf2e5504b3e93d72bf2be331744","source_hash":"8ad07975","execution_start":1681230726685,"execution_millis":2180,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\nWARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\nINFO:tensorflow:Assets written to: ./saved_models/hmw_20230411-154544/assets\nINFO:tensorflow:Assets written to: ./saved_models/hmw_20230411-154544/assets\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"## Tflite model conversion","metadata":{"cell_id":"6cd7741321d445d7a66348140351da20","formattedRanges":[],"deepnote_cell_type":"text-cell-h2"}},{"cell_type":"code","source":"converter = tf.lite.TFLiteConverter.from_saved_model(f'./saved_models/{MODEL_NAME}')\ntflite_model = converter.convert()","metadata":{"cell_id":"88df7683020541e48b37bd584df7f398","source_hash":"d18d3058","execution_start":1681230728746,"execution_millis":559,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stderr","text":"2023-04-11 16:32:08.891275: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n2023-04-11 16:32:08.891327: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n2023-04-11 16:32:08.892443: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: ./saved_models/hmw_20230411-154544\n2023-04-11 16:32:08.898415: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n2023-04-11 16:32:08.898456: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: ./saved_models/hmw_20230411-154544\n2023-04-11 16:32:08.925431: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n2023-04-11 16:32:08.927279: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.\n2023-04-11 16:32:08.979600: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: ./saved_models/hmw_20230411-154544\n2023-04-11 16:32:08.997253: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 104820 microseconds.\n2023-04-11 16:32:09.079894: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"tflite_models_dir = './tflite_models'\nif not os.path.exists(tflite_models_dir):\n    os.makedirs(tflite_models_dir)","metadata":{"tags":[],"cell_id":"0cedf78789574f66b54b3f4862bc4785","source_hash":"5eb9fb99","execution_start":1681230729245,"execution_millis":60,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":27},{"cell_type":"code","source":"tflite_model_name = os.path.join(tflite_models_dir, f'{MODEL_NAME}.tflite')\ntflite_model_name","metadata":{"cell_id":"27275cf7582d43edb5ef6dc4b55d665c","source_hash":"9658f457","execution_start":1681230729245,"execution_millis":59,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"output_type":"execute_result","execution_count":28,"data":{"text/plain":"'./tflite_models/hmw_20230411-154544.tflite'"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"#Save the tflite model\nwith open(tflite_model_name, 'wb') as fp:\n    fp.write(tflite_model)","metadata":{"cell_id":"44ec66173b484ad7b9d506b62c10d5ac","source_hash":"1d5d631b","execution_start":1681230729246,"execution_millis":60,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":29},{"cell_type":"code","source":"#Zip the tflite model\nimport zipfile\n\nwith zipfile.ZipFile(f'{tflite_model_name}.zip', 'w', compression=zipfile.ZIP_DEFLATED) as f:\n    f.write(tflite_model_name)","metadata":{"cell_id":"5a0f6df0ba654f8bbb92ec10a7fe1be4","source_hash":"1e64cc35","execution_start":1681230729265,"execution_millis":40,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":30},{"cell_type":"code","source":"#Check the size\ntflite_size = os.path.getsize(tflite_model_name) / 1024.0\nzipped_size = os.path.getsize(f'{tflite_model_name}.zip') / 1024.0\n\nprint(f'Original tflite size: {tflite_size:.3f} KB')\nprint(f'Zipped tflite size: {zipped_size:.3f} KB')","metadata":{"cell_id":"6dda86ec72154508a7865e9b9240101e","source_hash":"de82d504","execution_start":1681230729299,"execution_millis":7,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Original tflite size: 44.359 KB\nZipped tflite size: 21.095 KB\n","output_type":"stream"}],"execution_count":31},{"cell_type":"markdown","source":"## Test the tflite model latency","metadata":{"cell_id":"137e2031b8584341b115f5113767e41a","formattedRanges":[],"deepnote_cell_type":"text-cell-h2"}},{"cell_type":"code","source":"model_path = tflite_model_name\n#------------Get the model-----------------\ninterpreter = tf.lite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\nprint(\"Input name:\", input_details[0]['name'])\nprint(\"Input shape:\", input_details[0]['shape'])\nprint(\"Output name:\", output_details[0]['name'])\nprint(\"Output shape:\", output_details[0]['shape'])","metadata":{"cell_id":"57b2b6fdcc8f496f940a307654a7a63e","source_hash":"3618fc02","execution_start":1681230729324,"execution_millis":55,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Input name: serving_default_input_4:0\nInput shape: [  1 122  10   1]\nOutput name: StatefulPartitionedCall:0\nOutput shape: [1 3]\nINFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"from tqdm import tqdm\n\n#------------File preparation------------------\nfilenames = glob('MAVD-traffic/multilabel_test/*') \n\n#PREPROCESSING_MFCCS_ARGS is the one defined in the Hyperparameters section\n\n#-----------Compute the linear to mel weight matrix--------------------\ndownsampling_rate = PREPROCESSING_MFCCS_ARGS['downsampling_rate']\nsampling_rate_int64 = tf.cast(downsampling_rate, tf.int64)\nframe_length = int(downsampling_rate * PREPROCESSING_MFCCS_ARGS['frame_length_in_s'])\nframe_step = int(downsampling_rate * PREPROCESSING_MFCCS_ARGS['frame_step_in_s'])\nspectrogram_width = (16000 - frame_length) // frame_step + 1\nnum_spectrogram_bins = frame_length // 2 + 1\nnum_mfccs_coefficients = PREPROCESSING_MFCCS_ARGS['num_coefficients']\n\nlinear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(\n    PREPROCESSING_MFCCS_ARGS['num_mel_bins'],\n    num_spectrogram_bins,\n    downsampling_rate,\n    PREPROCESSING_MFCCS_ARGS['lower_frequency'],\n    PREPROCESSING_MFCCS_ARGS['upper_frequency']\n)\n\n\navg_preprocessing_latency = 0.0\navg_model_latency = 0.0\nlatencies = []\n\n#------------Test the model----------------\nfor filename in tqdm(filenames):\n    audio_binary = tf.io.read_file(filename)\n    sampling_rate = 44100\n    path_parts = tf.strings.split(filename, '/')\n    path_end = path_parts[-1]\n    file_parts = tf.strings.split(path_end, '_')\n    label =file_parts[:-1].numpy().astype(int)\n\n    \n    start_preprocess = time()\n\n    audio = tfio.audio.decode_flac(audio_binary, dtype=tf.int32)\n    audio = tf.squeeze(audio)\n    audio = tf.cast(audio, tf.float32)\n    stft = tf.signal.stft(\n        audio,\n        frame_length = frame_length,\n        frame_step = frame_step,\n        fft_length = frame_length\n    )\n\n    spectrogram = tf.abs(stft)\n\n    mel_spectrogram = tf.matmul(spectrogram, linear_to_mel_weight_matrix)\n    log_mel_spectrogram = tf.math.log(mel_spectrogram+1.e-6)\n    mfccs = tf.signal.mfccs_from_log_mel_spectrograms(log_mel_spectrogram)\n    mfccs = mfccs[:,:num_mfccs_coefficients]\n    mfccs = tf.expand_dims(mfccs, 0)\n    mfccs = tf.expand_dims(mfccs, -1)\n\n    end_preprocess = time()\n\n\n    interpreter.set_tensor(input_details[0]['index'], mfccs)\n    interpreter.invoke()\n    output = interpreter.get_tensor(output_details[0]['index'])\n\n    end_inference = time()\n\n    latencies.append(end_inference - start_preprocess)\n\n#-----------------Compute the latency----------------------------\nmedian_total_latency = np.median(latencies)\nmedian_total_latency\n","metadata":{"cell_id":"bdebf6416e9a49dbbebbf78856f278a9","source_hash":"afa701d7","execution_start":1681230729324,"execution_millis":396719,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stderr","text":"100%|██████████| 4990/4990 [06:36<00:00, 12.58it/s]\n","output_type":"stream"},{"output_type":"execute_result","execution_count":33,"data":{"text/plain":"0.06420135498046875"},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"model.summary()","metadata":{"cell_id":"97ace8cef5204014be0f843404a60b72","source_hash":"4e6a3b95","execution_start":1681222524670,"execution_millis":5412987,"deepnote_to_be_reexecuted":true,"deepnote_cell_type":"code"},"outputs":[],"execution_count":0},{"cell_type":"code","source":"","metadata":{"cell_id":"5dd829f128164f259b8a503f239e637d","source_hash":"b623e53d","deepnote_to_be_reexecuted":true,"deepnote_cell_type":"code"},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=251b70cc-64ed-4269-9639-75a0d233eb14' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz4KPHN2ZyB3aWR0aD0iODBweCIgaGVpZ2h0PSI4MHB4IiB2aWV3Qm94PSIwIDAgODAgODAiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+CiAgICA8IS0tIEdlbmVyYXRvcjogU2tldGNoIDU0LjEgKDc2NDkwKSAtIGh0dHBzOi8vc2tldGNoYXBwLmNvbSAtLT4KICAgIDx0aXRsZT5Hcm91cCAzPC90aXRsZT4KICAgIDxkZXNjPkNyZWF0ZWQgd2l0aCBTa2V0Y2guPC9kZXNjPgogICAgPGcgaWQ9IkxhbmRpbmciIHN0cm9rZT0ibm9uZSIgc3Ryb2tlLXdpZHRoPSIxIiBmaWxsPSJub25lIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiPgogICAgICAgIDxnIGlkPSJBcnRib2FyZCIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoLTEyMzUuMDAwMDAwLCAtNzkuMDAwMDAwKSI+CiAgICAgICAgICAgIDxnIGlkPSJHcm91cC0zIiB0cmFuc2Zvcm09InRyYW5zbGF0ZSgxMjM1LjAwMDAwMCwgNzkuMDAwMDAwKSI+CiAgICAgICAgICAgICAgICA8cG9seWdvbiBpZD0iUGF0aC0yMCIgZmlsbD0iIzAyNjVCNCIgcG9pbnRzPSIyLjM3NjIzNzYyIDgwIDM4LjA0NzY2NjcgODAgNTcuODIxNzgyMiA3My44MDU3NTkyIDU3LjgyMTc4MjIgMzIuNzU5MjczOSAzOS4xNDAyMjc4IDMxLjY4MzE2ODMiPjwvcG9seWdvbj4KICAgICAgICAgICAgICAgIDxwYXRoIGQ9Ik0zNS4wMDc3MTgsODAgQzQyLjkwNjIwMDcsNzYuNDU0OTM1OCA0Ny41NjQ5MTY3LDcxLjU0MjI2NzEgNDguOTgzODY2LDY1LjI2MTk5MzkgQzUxLjExMjI4OTksNTUuODQxNTg0MiA0MS42NzcxNzk1LDQ5LjIxMjIyODQgMjUuNjIzOTg0Niw0OS4yMTIyMjg0IEMyNS40ODQ5Mjg5LDQ5LjEyNjg0NDggMjkuODI2MTI5Niw0My4yODM4MjQ4IDM4LjY0NzU4NjksMzEuNjgzMTY4MyBMNzIuODcxMjg3MSwzMi41NTQ0MjUgTDY1LjI4MDk3Myw2Ny42NzYzNDIxIEw1MS4xMTIyODk5LDc3LjM3NjE0NCBMMzUuMDA3NzE4LDgwIFoiIGlkPSJQYXRoLTIyIiBmaWxsPSIjMDAyODY4Ij48L3BhdGg+CiAgICAgICAgICAgICAgICA8cGF0aCBkPSJNMCwzNy43MzA0NDA1IEwyNy4xMTQ1MzcsMC4yNTcxMTE0MzYgQzYyLjM3MTUxMjMsLTEuOTkwNzE3MDEgODAsMTAuNTAwMzkyNyA4MCwzNy43MzA0NDA1IEM4MCw2NC45NjA0ODgyIDY0Ljc3NjUwMzgsNzkuMDUwMzQxNCAzNC4zMjk1MTEzLDgwIEM0Ny4wNTUzNDg5LDc3LjU2NzA4MDggNTMuNDE4MjY3Nyw3MC4zMTM2MTAzIDUzLjQxODI2NzcsNTguMjM5NTg4NSBDNTMuNDE4MjY3Nyw0MC4xMjg1NTU3IDM2LjMwMzk1NDQsMzcuNzMwNDQwNSAyNS4yMjc0MTcsMzcuNzMwNDQwNSBDMTcuODQzMDU4NiwzNy43MzA0NDA1IDkuNDMzOTE5NjYsMzcuNzMwNDQwNSAwLDM3LjczMDQ0MDUgWiIgaWQ9IlBhdGgtMTkiIGZpbGw9IiMzNzkzRUYiPjwvcGF0aD4KICAgICAgICAgICAgPC9nPgogICAgICAgIDwvZz4KICAgIDwvZz4KPC9zdmc+' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote":{},"orig_nbformat":2,"deepnote_notebook_id":"fb0569439a2f433ebe6741301c3851f6","deepnote_execution_queue":[]}}